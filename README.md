# mmWave Radar Human Identification Platform

A PyTorch-based platform for training and evaluating point cloud-based human identification models. This project provides both a **web-based GUI** and **command-line tools** for easy model training, inspired by the MMIDNet research paper.

<div align="center">

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-Educational-green.svg)](LICENSE)

**üöÄ Train models with GUI or command line ‚Ä¢ üìä Compare 3 architectures ‚Ä¢ üîí Privacy-preserving identification**

</div>

---

## üìã Table of Contents

- [Overview](#overview)
- [Getting Started](#getting-started)
  - [Option 1: GUI (Recommended for Beginners)](#option-1-gui-recommended-for-beginners)
  - [Option 2: Command Line (For Advanced Users)](#option-2-command-line-for-advanced-users)
- [Dataset Setup](#dataset-setup)
- [Model Architectures](#model-architectures)
- [Configuration](#configuration)
- [Project Structure](#project-structure)
- [Expected Results](#expected-results)
- [Troubleshooting](#troubleshooting)
- [References](#references)

---

## üéØ Overview

### What This Project Does

This platform trains deep learning models to **identify individuals** from sparse 3D point clouds generated by mmWave radar. Unlike camera-based systems, mmWave radar:

- ‚úÖ **Preserves privacy** - no facial features or detailed imagery
- ‚úÖ **Works in darkness** - independent of lighting conditions
- ‚úÖ **Sees through obstacles** - penetrates clothing and some materials
- ‚úÖ **Generates sparse data** - typically <200 points per frame

### Three Models Included

| Model | Accuracy | Training Time | Best For |
|-------|----------|---------------|----------|
| **MLP Baseline** | ~20-40% | 10-20 min | Quick testing |
| **1D-CNN** | ~65-70% | 30-60 min | Balanced performance |
| **** ‚≠ê | ~70-85% | 60-120 min | Best accuracy |

### Use Cases

- Indoor human sensing for smart homes
- Security and access control without cameras
- Healthcare monitoring (fall detection, activity recognition)
- Privacy-preserving retail analytics

---

## üöÄ Getting Started

You can use this platform in **two ways**: via an intuitive **web GUI** or through **command-line tools**.

### Option 1: GUI (Recommended for Beginners)

The GUI provides a visual interface to configure, train, and evaluate models‚Äîno coding required!

#### Prerequisites

- **Docker** and **Docker Compose** installed ([Install Docker](https://docs.docker.com/get-docker/))
- **8GB+ RAM** recommended
- FAUST dataset (see [Dataset Setup](#dataset-setup))

#### Quick Start

```bash
# 1. Navigate to project directory
cd arqaios-take-home-assignment

# 2. Place FAUST .ply files in data/raw/
#    (See Dataset Setup section below)

# 3. Start the GUI application
bash start.sh

# 4. Open your browser to:
#    http://localhost:8080
```

That's it! The GUI will guide you through:
1. **Data Upload** - Drag & drop mesh files
2. **Preprocessing** - Configure sampling and augmentation
3. **Training** - Select model and hyperparameters
4. **Monitoring** - Watch real-time training progress
5. **Evaluation** - Generate performance reports
6. **Download** - Save trained models and results

#### GUI Features

- üìä **Real-time Training Curves** - Live loss and accuracy plots
- üéõÔ∏è **Interactive Configuration** - Adjust hyperparameters with sliders
- üìà **Progress Monitoring** - Epoch-by-epoch metrics
- üíæ **One-Click Downloads** - Export models and reports
- üåê **Web-Based** - No local Python environment needed

For detailed GUI instructions, see **[GUI_GUIDE.md](GUI_GUIDE.md)**.

---

### Option 2: Command Line (For Advanced Users)

For researchers and developers who prefer scripting and automation.

#### Prerequisites

- **Python 3.8+**
- **CUDA GPU** (optional but recommended)
- **8GB+ RAM**

#### Installation

```bash
# 1. Navigate to project directory
cd arqaios-take-home-assignment

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download FAUST dataset
# Place .ply files in data/raw/
# (See Dataset Setup section below)
```

#### Basic Usage

**Train a single model:**

```bash
# Train PointNet (best accuracy)
python src/train.py --config config.yaml --model pointnet

# Train 1D-CNN (moderate performance)
python src/train.py --config config.yaml --model cnn1d

# Train MLP (baseline)
python src/train.py --config config.yaml --model mlp
```

**Train all three models:**

```bash
bash train_all_models.sh
```

**Evaluate a model:**

```bash
python src/evaluate.py \
    --model pointnet \
    --checkpoint results/checkpoints/pointnet/model_best.pth
```

**Compare all models:**

```bash
python src/evaluate.py --compare --models mlp cnn1d pointnet
```

**Monitor training with TensorBoard:**

```bash
tensorboard --logdir results/tensorboard
# Open http://localhost:6006 in your browser
```

#### Command-Line Options

**Training options:**
```bash
python src/train.py \
    --config config.yaml \        # Configuration file
    --model pointnet \             # Model type: mlp, cnn1d, pointnet
    --resume checkpoint.pth        # Resume from checkpoint (optional)
```

**Evaluation options:**
```bash
python src/evaluate.py \
    --model pointnet \             # Model type
    --checkpoint model.pth \       # Model checkpoint path
    --compare \                    # Compare multiple models
    --models mlp cnn1d pointnet    # Models to compare
```

For detailed command-line usage, see **[QUICKSTART.md](QUICKSTART.md)**.

---

## üìä Dataset Setup

### FAUST Dataset

This project uses the **FAUST (Faces of Articulated Super-humans Through Training)** dataset:

- **10 subjects** √ó **10 poses** = 100 mesh files
- Each mesh is sampled to **200 points** √ó **3 coordinates** (x, y, z)
- Augmented to **10,000 training samples**

### Download Instructions

1. **Visit the FAUST website**: http://faust.is.tue.mpg.de/
2. **Download** the "MPI FAUST Training" dataset
3. **Extract** the `.ply` or `.obj` files
4. **Place files** in `data/raw/` directory

Your directory should look like:
```
data/raw/
  ‚îú‚îÄ‚îÄ tr_reg_000.ply
  ‚îú‚îÄ‚îÄ tr_reg_001.ply
  ‚îú‚îÄ‚îÄ tr_reg_002.ply
  ...
  ‚îî‚îÄ‚îÄ tr_reg_099.ply
```

### Data Processing

The first time you run training, the platform will automatically:
1. Load mesh files from `data/raw/`
2. Sample 200 points per mesh using Farthest Point Sampling (FPS)
3. Create 100 augmented samples per mesh
4. Normalize to unit sphere
5. Save processed data to `data/processed/faust_pc.npz`

**Augmentation techniques:**
- Random rotation (0-360¬∞ around z-axis)
- Random translation (¬±0.2m in x-y plane)
- Normalization to unit sphere

---

## üèóÔ∏è Model Architectures

### 1. MLP Baseline

**Architecture:**
```
Input (200, 3) ‚Üí Flatten (600) ‚Üí FC(256) ‚Üí FC(128) ‚Üí Output (10)
```

**Characteristics:**
- ‚ö° **Fast training**: 10-20 minutes
- üìâ **Low accuracy**: ~20-40%
- ‚ùå **Order-dependent**: Sensitive to point ordering
- ‚úÖ **Good for testing**: Verifies data pipeline works

**When to use**: Quick sanity check before training larger models.

---

### 2. 1D-CNN Model

**Architecture:**
```
Input (200, 3) ‚Üí Sort ‚Üí Conv1D(64) ‚Üí Conv1D(128) ‚Üí Conv1D(256) 
               ‚Üí GlobalMaxPool ‚Üí FC(128) ‚Üí Output (10)
```

**Characteristics:**
- ‚ö° **Moderate training**: 30-60 minutes
- üìä **Good accuracy**: ~65-70%
- ‚úì **Local patterns**: Captures spatial relationships
- ‚ö†Ô∏è **Partially order-invariant**: Depends on sorting strategy

**When to use**: Balanced performance without heavy computation.

---

### 3.  ‚≠ê (Recommended)

**Architecture:**
```
Input (200, 3) ‚Üí T-Net ‚Üí Transform ‚Üí Shared MLP (64, 128, 1024)
               ‚Üí GlobalMaxPool ‚Üí FC(512) ‚Üí FC(256) ‚Üí Output (10)
```

**Characteristics:**
- üöÄ **Best accuracy**: ~70-85%
- ‚è±Ô∏è **Longer training**: 60-120 minutes
- ‚úÖ **Permutation-invariant**: Order doesn't matter
- ‚úÖ **Rotation-invariant**: T-Net learns alignment
- üéØ **State-of-the-art**: Based on PointNet paper

**Key components:**
- **T-Net**: Learns 3√ó3 transformation matrix for canonical alignment
- **Shared MLP**: Processes each point independently
- **Global Max Pooling**: Symmetric aggregation function

**When to use**: When you need the best possible accuracy.

---

## üîß Configuration

The platform is configured via `config.yaml`. Here are the key settings:

### Data Configuration

```yaml
data:
  num_points: 200                # Points per sample
  samples_per_mesh: 100          # Augmented samples per mesh
  normalize_center: true         # Center point cloud
  normalize_scale: true          # Scale to unit sphere
  raw_dir: data/raw
  processed_dir: data/processed
```

### Training Configuration

```yaml
training:
  batch_size: 64                 # Samples per batch
  num_epochs: 120                # Maximum training epochs
  learning_rate: 0.001           # Initial learning rate
  weight_decay: 0.0001           # L2 regularization
  early_stopping_patience: 20    # Stop if no improvement
```

### Model Configuration

```yaml
model:
  type: pointnet                 # mlp, cnn1d, or pointnet
  num_classes: 10                # Number of subjects
  dropout: 0.2                   # Dropout probability
```

### Augmentation Configuration

```yaml
augmentation:
  rotation_range: 30             # Rotation in degrees
  translation_range: 0.2         # Translation in meters
  normalize: true                # Apply normalization
```

### Tuning Tips

- **Increase batch_size** (e.g., 128) ‚Üí Faster training, requires more memory
- **Decrease learning_rate** (e.g., 0.0005) ‚Üí More stable but slower convergence
- **Increase dropout** (e.g., 0.3) ‚Üí Reduces overfitting
- **Adjust augmentation** ‚Üí More augmentation = better generalization

---

## üìÅ Project Structure

```
arqaios-take-home-assignment/
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ config.yaml                    # Configuration file
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies (CLI)
‚îú‚îÄ‚îÄ requirements-gui.txt           # Additional GUI dependencies
‚îú‚îÄ‚îÄ start.sh                       # Quick start script for GUI
‚îú‚îÄ‚îÄ train_all_models.sh            # Train all models script
‚îú‚îÄ‚îÄ docker-compose.yml             # Docker Compose configuration
‚îú‚îÄ‚îÄ Dockerfile                     # Docker image definition
‚îÇ
‚îú‚îÄ‚îÄ frontend/                      # Web GUI frontend
‚îÇ   ‚îú‚îÄ‚îÄ index.html                 # Main HTML page
‚îÇ   ‚îú‚îÄ‚îÄ app.js                     # Frontend JavaScript
‚îÇ   ‚îî‚îÄ‚îÄ style.css                  # Styling
‚îÇ
‚îú‚îÄ‚îÄ backend/                       # Flask backend server
‚îÇ   ‚îú‚îÄ‚îÄ app.py                     # Main Flask application
‚îÇ   ‚îú‚îÄ‚îÄ training_manager.py        # Training job management
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ src/                           # Core ML code
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py           # Data augmentation & normalization
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                 # PyTorch Dataset & DataLoader
‚îÇ   ‚îú‚îÄ‚îÄ train.py                   # Training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                # Evaluation & visualization
‚îÇ   ‚îî‚îÄ‚îÄ models/                    # Model implementations
‚îÇ       ‚îú‚îÄ‚îÄ mlp.py                 # MLP Baseline
‚îÇ       ‚îú‚îÄ‚îÄ cnn1d.py               # 1D-CNN Model
‚îÇ       ‚îî‚îÄ‚îÄ pointnet_tiny.py       # 
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Raw mesh files (.ply, .obj)
‚îÇ   ‚îî‚îÄ‚îÄ processed/                 # Preprocessed point clouds (.npz)
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Training outputs
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/               # Saved model weights
‚îÇ   ‚îú‚îÄ‚îÄ tensorboard/               # TensorBoard logs
‚îÇ   ‚îú‚îÄ‚îÄ reports/                   # Evaluation reports (JSON)
‚îÇ   ‚îî‚îÄ‚îÄ experiments/               # Experiment results & plots
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ eda.ipynb                  # Exploratory Data Analysis
‚îÇ
‚îî‚îÄ‚îÄ docs/                          # Documentation
    ‚îú‚îÄ‚îÄ GUI_GUIDE.md               # Detailed GUI instructions
    ‚îú‚îÄ‚îÄ QUICKSTART.md              # Command-line quickstart
    ‚îú‚îÄ‚îÄ EXPERIMENT_GUIDE.md        # Advanced experiments
    ‚îî‚îÄ‚îÄ TECHNICAL_REPORT.md        # Full technical report
```

---

## üìà Expected Results

### Performance Metrics

| Model | Test Accuracy | F1-Score | Parameters | Training Time |
|-------|--------------|----------|------------|---------------|
| MLP Baseline | 35-40% | 0.33-0.38 | ~200K | 10-20 min |
| 1D-CNN | 65-70% | 0.63-0.68 | ~300K | 30-60 min |
| Tiny PointNet | 70-85% | 0.78-0.83 | ~1.5M | 60-120 min |

### What You'll Get

After training, you'll find:

üìÅ **Checkpoints** (`results/checkpoints/<model>/`)
- `model_best.pth` - Best model based on validation accuracy
- `model_epoch_N.pth` - Checkpoints every N epochs

üìä **TensorBoard Logs** (`results/tensorboard/<model>/`)
- Training/validation loss curves
- Training/validation accuracy curves
- Real-time metrics during training

üìà **Evaluation Reports** (`results/reports/`)
- JSON files with detailed metrics
- Confusion matrices
- Per-class precision/recall/F1

üìâ **Visualizations** (`results/experiments/`)
- `model_comparison.png` - Side-by-side comparison
- `confusion_matrix_<model>.png` - Confusion matrices
- `performance_curves.png` - Training curves

### Interpreting Results

**Good signs:**
- ‚úÖ Training loss decreases steadily
- ‚úÖ Validation accuracy improves over time
- ‚úÖ Small gap between train and validation accuracy

**Warning signs:**
- ‚ö†Ô∏è Validation accuracy plateaus early ‚Üí Increase model capacity
- ‚ö†Ô∏è Large train-val gap ‚Üí Reduce overfitting (increase dropout)
- ‚ö†Ô∏è Both accuracies low ‚Üí Check data preprocessing

---

## üîß Troubleshooting

### Common Issues

#### Issue: "No mesh files found in data/raw/"

**Solution:**
1. Download FAUST dataset from http://faust.is.tue.mpg.de/
2. Extract `.ply` or `.obj` files
3. Place in `data/raw/` directory
4. Verify with: `ls data/raw/*.ply`

---

#### Issue: "CUDA out of memory"

**Solutions:**
1. **Reduce batch size** in `config.yaml`: `batch_size: 32` or `16`
2. **Reduce num_points**: `num_points: 100`
3. **Use CPU**: Set `device: cpu` in `config.yaml`
4. **Close other GPU applications**

---

#### Issue: "Training is very slow"

**Solutions:**
1. **Use GPU**: Ensure CUDA is available with `python -c "import torch; print(torch.cuda.is_available())"`
2. **Increase batch size**: `batch_size: 128` (if memory allows)
3. **Use fewer augmentations**: Reduce `samples_per_mesh`
4. **Train smaller model first**: Start with MLP to verify pipeline

---

#### Issue: "Model accuracy is stuck at ~10%"

**Solutions:**
1. **Check data**: Verify meshes loaded correctly
2. **Reduce learning rate**: Try `learning_rate: 0.0001`
3. **Check device**: Ensure model and data on same device
4. **Increase training time**: Set `num_epochs: 200`

---

#### Issue: "Docker container won't start"

**Solutions:**
1. **Check Docker status**: `docker ps -a`
2. **View logs**: `docker-compose logs`
3. **Rebuild**: `docker-compose down && docker-compose up --build`
4. **Check port**: Ensure port 8080 not in use

---

### Getting Help

1. **Check logs**: 
   - GUI: View training logs in the web interface
   - CLI: Check console output and TensorBoard
2. **Review documentation**:
   - [GUI_GUIDE.md](GUI_GUIDE.md) for GUI issues
   - [QUICKSTART.md](QUICKSTART.md) for CLI issues
3. **Examine code comments**: All functions have detailed docstrings
4. **Check configuration**: Verify `config.yaml` settings

---

## üìö References

### Research Papers

1. **MMIDNet Paper**: "Human Identification Using mmWave Radar" (MECO 2024)
   - Original research this project is based on
   - Achieves 92.4% accuracy on 12 subjects
   - Uses temporal modeling with Bi-LSTM

2. **PointNet**: "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" (CVPR 2017)
   - Introduces permutation-invariant architecture for point clouds
   - T-Net for spatial transformation learning
   - [arXiv:1612.00593](https://arxiv.org/abs/1612.00593)

3. **FAUST Dataset**: "FAUST: Dataset and Evaluation for 3D Mesh Registration" (CVPR 2014)
   - 100 human body meshes (10 subjects √ó 10 poses)
   - http://faust.is.tue.mpg.de/

### Key Concepts

- **Permutation Invariance**: Output unchanged by point ordering
- **Farthest Point Sampling (FPS)**: Better shape preservation than random sampling
- **T-Net**: Learns canonical alignment transformation
- **mmWave Radar**: 77-81 GHz frequency; sparse 3D point clouds

### External Resources

- **PyTorch Documentation**: https://pytorch.org/docs/
- **TensorBoard Guide**: https://www.tensorflow.org/tensorboard
- **Docker Documentation**: https://docs.docker.com/

---

## ü§ù Acknowledgments

- **Arqaios**: For the take-home assignment opportunity
- **MPI FAUST**: For providing the human body mesh dataset
- **PointNet Authors**: For the foundational architecture

---

## üìÑ License

This project is for **educational and research purposes only**. Please cite the original MMIDNet paper if using this work academically.

---

## üöÄ Quick Command Reference

### GUI Usage
```bash
bash start.sh                              # Start GUI
docker-compose logs -f                     # View logs
docker-compose down                        # Stop GUI
```

### Command-Line Usage
```bash
# Training
python src/train.py --model pointnet       # Train single model
bash train_all_models.sh                   # Train all models

# Evaluation
python src/evaluate.py --model pointnet \
  --checkpoint results/checkpoints/pointnet/model_best.pth

# Monitoring
tensorboard --logdir results/tensorboard


