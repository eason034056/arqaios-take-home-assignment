"""
Unified Experiment Results Analysis Script
=================================================================

This script analyzes all experiment results generated by run_experiments.sh.

Experiment Design:
  - 2 centering options: With / Without
  - 3 models: MLP, CNN1D (k=3), PointNet
  - Total: 6 experiments

Outputs:
  1. summary_report.txt - detailed text report
  2. performance_comparison.png - performance comparison plot
  3. centering_impact.png - centering impact analysis plot
  4. model_ranking.png - model ranking plot
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from pathlib import Path
from typing import Dict, Optional

# Set style for better-looking plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 10

# ================================================================
# Helper Functions
# ================================================================

def extract_best_results(model_dir: Path) -> Optional[Dict[str, float]]:
    """
    Extract best validation results from model directory
    
    Args:
        model_dir: path to model results directory
        
    Returns:
        dict with val_acc, val_loss, epoch, or None if not found
    """
    checkpoint_path = model_dir / 'checkpoints' / 'model_best.pth'
    
    if not checkpoint_path.exists():
        print(f"âš  Warning: {checkpoint_path} not found")
        return None
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        return {
            'val_acc': checkpoint.get('val_acc', 0.0),
            'val_loss': checkpoint.get('val_loss', 0.0),
            'epoch': checkpoint.get('epoch', 0),
            'train_acc': checkpoint.get('train_acc', 0.0)
        }
    except Exception as e:
        print(f"âœ— Error loading {checkpoint_path}: {e}")
        return None


def parse_all_results(results_dir: Path) -> pd.DataFrame:
    """
    Parse all experiment results
    
    Args:
        results_dir: root directory of experiment results
        
    Returns:
        DataFrame containing all experiment results
    """
    results = []
    
    models = ['mlp', 'cnn1d', 'pointnet']
    center_settings = ['with_center', 'no_center']
    
    for center in center_settings:
        for model in models:
            dir_name = f"{center}_{model}"
            model_dir = results_dir / dir_name
            
            if model_dir.exists():
                metrics = extract_best_results(model_dir)
                if metrics:
                    # Model display name
                    model_display = {
                        'mlp': 'MLP',
                        'cnn1d': 'CNN1D (k=3)',
                        'pointnet': 'PointNet'
                    }[model]
                    
                    # Centering display name
                    center_display = 'With Centering' if center == 'with_center' else 'No Centering'
                    
                    results.append({
                        'model': model_display,
                        'model_type': model,
                        'centering': center_display,
                        'center_type': center,
                        'val_acc': metrics['val_acc'],
                        'train_acc': metrics['train_acc'],
                        'val_loss': metrics['val_loss'],
                        'best_epoch': metrics['epoch']
                    })
                else:
                    print(f"âš  No results found for: {dir_name}")
            else:
                print(f"âš  Directory not found: {model_dir}")
    
    return pd.DataFrame(results)


# ================================================================
# Visualization Functions
# ================================================================

def create_performance_comparison(df: pd.DataFrame, output_dir: Path):
    """
    Create performance comparison plot
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    
    # Pivot data for grouped bar chart
    pivot = df.pivot_table(
        index='model',
        columns='centering',
        values='val_acc'
    )
    
    # Create grouped bar chart
    pivot.plot(kind='bar', ax=ax, width=0.7, color=['#4ECDC4', '#FF6B6B'])
    
    ax.set_xlabel('Model', fontsize=12, fontweight='bold')
    ax.set_ylabel('Validation Accuracy (%)', fontsize=12, fontweight='bold')
    ax.set_title('Performance Comparison: Impact of Centering', 
                 fontsize=14, fontweight='bold')
    ax.legend(title='Data Processing', fontsize=10)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels on bars
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f%%', padding=3, fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_dir / 'performance_comparison.png'}")
    plt.close()


def create_centering_impact_plot(df: pd.DataFrame, output_dir: Path):
    """
    Create centering impact analysis plot
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Calculate impact of centering for each model
    impact_data = []
    for model in df['model'].unique():
        with_center = df[(df['model'] == model) & (df['center_type'] == 'with_center')]['val_acc'].values
        no_center = df[(df['model'] == model) & (df['center_type'] == 'no_center')]['val_acc'].values
        
        if len(with_center) > 0 and len(no_center) > 0:
            diff = with_center[0] - no_center[0]
            impact_data.append({
                'model': model,
                'with_center': with_center[0],
                'no_center': no_center[0],
                'improvement': diff
            })
    
    impact_df = pd.DataFrame(impact_data)
    
    # Create bar chart showing improvement
    colors = ['#4ECDC4' if x > 0 else '#FF6B6B' for x in impact_df['improvement']]
    bars = ax.bar(range(len(impact_df)), impact_df['improvement'], color=colors, alpha=0.7)
    
    ax.set_xticks(range(len(impact_df)))
    ax.set_xticklabels(impact_df['model'], fontsize=11)
    ax.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')
    ax.set_title('Impact of Centering on Model Performance\n(Positive = Centering Helps)', 
                 fontsize=13, fontweight='bold')
    ax.axhline(y=0, color='black', linestyle='--', linewidth=1)
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, impact_df['improvement'])):
        ax.text(i, val + 0.5 if val > 0 else val - 0.5, f'{val:+.2f}%', 
                ha='center', fontweight='bold', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'centering_impact.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_dir / 'centering_impact.png'}")
    plt.close()


def create_model_ranking_plot(df: pd.DataFrame, output_dir: Path):
    """
    Create model ranking plot
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Ranking for "With Centering"
    with_center_df = df[df['center_type'] == 'with_center'].sort_values('val_acc', ascending=False)
    colors_with = ['#FFD700', '#C0C0C0', '#CD7F32'][:len(with_center_df)]
    
    ax1.barh(range(len(with_center_df)), with_center_df['val_acc'], color=colors_with, alpha=0.7)
    ax1.set_yticks(range(len(with_center_df)))
    ax1.set_yticklabels(with_center_df['model'])
    ax1.set_xlabel('Validation Accuracy (%)', fontsize=11, fontweight='bold')
    ax1.set_title('Model Ranking: WITH Centering', fontsize=12, fontweight='bold')
    ax1.invert_yaxis()
    ax1.grid(axis='x', alpha=0.3)
    
    # Add value labels
    for i, val in enumerate(with_center_df['val_acc']):
        ax1.text(val + 1, i, f'{val:.2f}%', va='center', fontsize=9, fontweight='bold')
    
    # Ranking for "No Centering"
    no_center_df = df[df['center_type'] == 'no_center'].sort_values('val_acc', ascending=False)
    colors_no = ['#FFD700', '#C0C0C0', '#CD7F32'][:len(no_center_df)]
    
    ax2.barh(range(len(no_center_df)), no_center_df['val_acc'], color=colors_no, alpha=0.7)
    ax2.set_yticks(range(len(no_center_df)))
    ax2.set_yticklabels(no_center_df['model'])
    ax2.set_xlabel('Validation Accuracy (%)', fontsize=11, fontweight='bold')
    ax2.set_title('Model Ranking: NO Centering', fontsize=12, fontweight='bold')
    ax2.invert_yaxis()
    ax2.grid(axis='x', alpha=0.3)
    
    # Add value labels
    for i, val in enumerate(no_center_df['val_acc']):
        ax2.text(val + 1, i, f'{val:.2f}%', va='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'model_ranking.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved: {output_dir / 'model_ranking.png'}")
    plt.close()


# ================================================================
# Report Generation Function
# ================================================================

def generate_summary_report(df: pd.DataFrame, output_dir: Path):
    """
    Generate detailed text summary report
    """
    report_path = output_dir / 'summary_report.txt'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("Unified Experiment Results Summary Report\n")
        f.write("=" * 100 + "\n\n")
        
        # Experiment design
        f.write("Experiment Design:\n")
        f.write("-" * 100 + "\n")
        f.write("This experiment tests the impact of centering on three models\n\n")
        f.write("Centering Options:\n")
        f.write("  1. With Centering: move point cloud center to (0,0,0) (keep body size)\n")
        f.write("  2. No Centering: keep original coordinates and body size\n\n")
        f.write("Models:\n")
        f.write("  1. MLP: Multi-Layer Perceptron Baseline\n")
        f.write("  2. CNN1D (k=3): 1D CNN with kernel_size=3 for local patterns\n")
        f.write("  3. PointNet: PointNet with T-Net for transformation invariance\n\n")
        f.write("Data Split: Grouped (No data leakage)\n")
        f.write("Total Experiments: 2 Ã— 3 = 6\n")
        f.write("=" * 100 + "\n\n")
        
        # Detailed results table
        f.write("DETAILED RESULTS:\n")
        f.write("-" * 100 + "\n")
        f.write(df.to_string(index=False))
        f.write("\n\n")
        
        # Key findings
        f.write("KEY FINDINGS:\n")
        f.write("=" * 100 + "\n\n")
        
        # 1. Overall best model
        f.write("1. OVERALL BEST MODEL:\n")
        f.write("-" * 100 + "\n")
        best_row = df.loc[df['val_acc'].idxmax()]
        f.write(f"  Model: {best_row['model']}\n")
        f.write(f"  Centering: {best_row['centering']}\n")
        f.write(f"  Validation Accuracy: {best_row['val_acc']:.2f}%\n")
        f.write(f"  Training Accuracy: {best_row['train_acc']:.2f}%\n")
        f.write(f"  Best Epoch: {best_row['best_epoch']}\n\n")
        
        # 2. Impact of centering
        f.write("2. IMPACT OF CENTERING:\n")
        f.write("-" * 100 + "\n")
        for model in df['model'].unique():
            with_center = df[(df['model'] == model) & (df['center_type'] == 'with_center')]['val_acc']
            no_center = df[(df['model'] == model) & (df['center_type'] == 'no_center')]['val_acc']
            
            if len(with_center) > 0 and len(no_center) > 0:
                diff = with_center.values[0] - no_center.values[0]
                f.write(f"\n{model}:\n")
                f.write(f"  With Centering: {with_center.values[0]:.2f}%\n")
                f.write(f"  No Centering: {no_center.values[0]:.2f}%\n")
                f.write(f"  Improvement: {diff:+.2f}%\n")
                
                if diff > 2:
                    f.write(f"  â†’ Centering significantly improves performance\n")
                elif diff < -2:
                    f.write(f"  â†’ Centering hurts performance\n")
                else:
                    f.write(f"  â†’ Centering has minimal impact\n")
        
        # 3. Model rankings
        f.write("\n3. MODEL RANKINGS:\n")
        f.write("-" * 100 + "\n")
        
        f.write("\nWith Centering:\n")
        with_center_df = df[df['center_type'] == 'with_center'].sort_values('val_acc', ascending=False)
        for rank, (idx, row) in enumerate(with_center_df.iterrows(), 1):
            f.write(f"  {rank}. {row['model']}: {row['val_acc']:.2f}%\n")
        
        f.write("\nNo Centering:\n")
        no_center_df = df[df['center_type'] == 'no_center'].sort_values('val_acc', ascending=False)
        for rank, (idx, row) in enumerate(no_center_df.iterrows(), 1):
            f.write(f"  {rank}. {row['model']}: {row['val_acc']:.2f}%\n")
        
        # 4. Overfitting analysis
        f.write("\n4. OVERFITTING ANALYSIS:\n")
        f.write("-" * 100 + "\n")
        df['overfitting_gap'] = df['train_acc'] - df['val_acc']
        for idx, row in df.iterrows():
            f.write(f"\n{row['model']} ({row['centering']}):\n")
            f.write(f"  Train-Val Gap: {row['overfitting_gap']:.2f}%\n")
            if row['overfitting_gap'] > 20:
                f.write(f"  â†’ Severe overfitting\n")
            elif row['overfitting_gap'] > 10:
                f.write(f"  â†’ Moderate overfitting\n")
            else:
                f.write(f"  â†’ Good generalization\n")
        
        f.write("\n" + "=" * 100 + "\n")
        f.write("END OF REPORT\n")
        f.write("=" * 100 + "\n")
    
    print(f"âœ“ Saved: {report_path}")


# ================================================================
# Main Function
# ================================================================

def main():
    """
    Main program entry point
    """
    results_dir = Path('results/experiments')
    
    if not results_dir.exists():
        print(f"âœ— Error: Results directory not found: {results_dir}")
        print("  Please run the experiment first: bash run_experiments.sh")
        return
    
    print("=" * 100)
    print("Analyzing Experiment Results")
    print("=" * 100)
    print()
    
    # Parse all results
    print("ğŸ“Š Parsing experiment results...")
    df = parse_all_results(results_dir)
    
    if df.empty:
        print("âœ— Error: No results found in the experiment directory")
        return
    
    print(f"âœ“ Found {len(df)} experiment results\n")
    print("Results Preview:")
    print(df.to_string(index=False))
    print()
    
    # Generate visualizations
    print("ğŸ“ˆ Generating visualizations...")
    create_performance_comparison(df, results_dir)
    create_centering_impact_plot(df, results_dir)
    create_model_ranking_plot(df, results_dir)
    print()
    
    # Generate summary report
    print("ğŸ“ Generating summary report...")
    generate_summary_report(df, results_dir)
    print()
    
    print("=" * 100)
    print("âœ“ Analysis Complete!")
    print("=" * 100)
    print()
    print("Generated Files:")
    print(f"  ğŸ“„ {results_dir}/summary_report.txt")
    print(f"  ğŸ“Š {results_dir}/performance_comparison.png")
    print(f"  ğŸ“Š {results_dir}/centering_impact.png")
    print(f"  ğŸ“Š {results_dir}/model_ranking.png")
    print()
    print("View Report:")
    print(f"  cat {results_dir}/summary_report.txt")
    print()
    print("View Plots:")
    print(f"  open {results_dir}/*.png")
    print()


if __name__ == '__main__':
    main()

